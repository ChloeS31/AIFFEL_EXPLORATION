{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "utility-burst",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recovered-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-yesterday",
   "metadata": {},
   "source": [
    "## Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "purple-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabModel(object):\n",
    "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "    INPUT_SIZE = 513\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "    def __init__(self, tarball_path):\n",
    "        self.graph = tf.Graph()\n",
    "        graph_def = None\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "        tar_file.close()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "    \t    tf.compat.v1.import_graph_def(graph_def, name='')\n",
    "\n",
    "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
    "\n",
    "    def preprocess(self, img_orig):\n",
    "        height, width = img_orig.shape[:2]\n",
    "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "        resized_image = cv2.resize(img_orig, target_size)\n",
    "        resized_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "        img_input = resized_rgb\n",
    "        return img_input\n",
    "        \n",
    "    def run(self, image):\n",
    "        img_input = self.preprocess(image)\n",
    "\n",
    "        batch_seg_map = self.sess.run(\n",
    "            self.OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={self.INPUT_TENSOR_NAME: [img_input]})\n",
    "\n",
    "        seg_map = batch_seg_map[0]\n",
    "        return cv2.cvtColor(img_input, cv2.COLOR_RGB2BGR), seg_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-correction",
   "metadata": {},
   "source": [
    "## Download & load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "equivalent-capture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp directory: /aiffel/aiffel/human_segmentation/models\n",
      "model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# define model and download & load pretrained weight\n",
    "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
    "\n",
    "model_dir = os.getenv('HOME')+'/aiffel/human_segmentation/models'\n",
    "tf.io.gfile.makedirs(model_dir)\n",
    "\n",
    "print ('temp directory:', model_dir)\n",
    "\n",
    "download_path = os.path.join(model_dir, 'deeplab_model.tar.gz')\n",
    "if not os.path.exists(download_path):\n",
    "    urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + 'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
    "                   download_path)\n",
    "\n",
    "MODEL = DeepLabModel(download_path)\n",
    "print('model loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-sheet",
   "metadata": {},
   "source": [
    "## 1) Person\n",
    "### Load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cellular-maine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(763, 610, 3)\n"
     ]
    }
   ],
   "source": [
    "img_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/박보영1.jpg'  # 본인이 선택한 이미지의 경로에 맞게 바꿔 주세요. \n",
    "img_orig = cv2.imread(img_path) \n",
    "print (img_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-idaho",
   "metadata": {},
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resized, seg_map = MODEL.run(img_orig)\n",
    "print (img_orig.shape, img_resized.shape, seg_map.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-roman",
   "metadata": {},
   "source": [
    "### Define Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "]\n",
    "len(LABEL_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-violation",
   "metadata": {},
   "source": [
    "### Define Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_show = img_resized.copy()\n",
    "seg_map = np.where(seg_map == 15, 15, 0) # 예측 중 사람만 추출\n",
    "img_mask = seg_map * (255/seg_map.max()) # 255 normalization\n",
    "img_mask = img_mask.astype(np.uint8)\n",
    "color_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n",
    "img_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.35, 0.0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-projector",
   "metadata": {},
   "source": [
    "### resize to the shape of original picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_up = cv2.resize(img_mask, img_orig.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)\n",
    "_, img_mask_up = cv2.threshold(img_mask_up, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.imshow(img_mask_up, cmap=plt.cm.binary_r)\n",
    "ax.set_title('Original Size Mask')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "plt.imshow(img_mask, cmap=plt.cm.binary_r)\n",
    "ax.set_title('DeepLab Model Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-billy",
   "metadata": {},
   "source": [
    "### blur picture for the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_orig_blur = cv2.blur(img_orig, (13,13)) #(13,13)은 blurring  kernel size를 뜻합니다. \n",
    "plt.imshow(cv2.cvtColor(img_orig_blur, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-enclosure",
   "metadata": {},
   "source": [
    "### get blurred background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_color = cv2.cvtColor(img_mask_up, cv2.COLOR_GRAY2BGR)\n",
    "img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
    "img_bg_blur = cv2.bitwise_and(img_orig_blur, img_bg_mask)\n",
    "plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-genre",
   "metadata": {},
   "source": [
    "### Concatenate original picture and blurred background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-inspection",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_concat = np.where(img_mask_color==255, img_orig, img_bg_blur)\n",
    "plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-building",
   "metadata": {},
   "source": [
    "### Try other pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-japanese",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### get image path\n",
    "img_dir = os.getenv('HOME')+'/aiffel/human_segmentation/images'  # 본인이 선택한 이미지의 경로에 맞게 바꿔 주세요. \n",
    "image_files= ['박보영2.jpg', '박보영3.png']\n",
    "for file_name in image_files:\n",
    "    print(file_name.split('.')[0])\n",
    "    img_path= os.path.join(img_dir, file_name)\n",
    "    img_orig = cv2.imread(img_path) \n",
    "    print (img_orig.shape)\n",
    "    ### run model\n",
    "    img_resized, seg_map = MODEL.run(img_orig)\n",
    "    print (img_orig.shape, img_resized.shape, seg_map.max())\n",
    "    ### Define Mask\n",
    "    img_show = img_resized.copy()\n",
    "    seg_map = np.where(seg_map == 15, 15, 0) # 예측 중 사람만 추출\n",
    "    img_mask = seg_map * (255/seg_map.max()) # 255 normalization\n",
    "    img_mask = img_mask.astype(np.uint8)\n",
    "    color_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n",
    "    img_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.35, 0.0)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    ### resize to the shape of original picture\n",
    "    img_mask_up = cv2.resize(img_mask, img_orig.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)\n",
    "    _, img_mask_up = cv2.threshold(img_mask_up, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    plt.imshow(img_mask_up, cmap=plt.cm.binary_r)\n",
    "    ax.set_title('Original Size Mask')\n",
    "\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    plt.imshow(img_mask, cmap=plt.cm.binary_r)\n",
    "    ax.set_title('DeepLab Model Mask')\n",
    "\n",
    "    plt.show()\n",
    "    ### blur picture for the background\n",
    "    img_orig_blur = cv2.blur(img_orig, (13,13)) #(13,13)은 blurring  kernel size를 뜻합니다. \n",
    "    plt.imshow(cv2.cvtColor(img_orig_blur, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    ### get blurred background\n",
    "    img_mask_color = cv2.cvtColor(img_mask_up, cv2.COLOR_GRAY2BGR)\n",
    "    img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
    "    img_bg_blur = cv2.bitwise_and(img_orig_blur, img_bg_mask)\n",
    "    plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    ### Concatenate original picture and blurred background\n",
    "    img_concat = np.where(img_mask_color==255, img_orig, img_bg_blur)\n",
    "    plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-grove",
   "metadata": {},
   "source": [
    "## 2) Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-public",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### get image path\n",
    "img_dir = os.getenv('HOME')+'/aiffel/human_segmentation/images'  # 본인이 선택한 이미지의 경로에 맞게 바꿔 주세요. \n",
    "image_files= ['cat1.jpg', 'cat2.jpg']\n",
    "for file_name in image_files:\n",
    "    print(file_name.split('.')[0])\n",
    "    img_path= os.path.join(img_dir, file_name)\n",
    "    img_orig = cv2.imread(img_path) \n",
    "    print (img_orig.shape)\n",
    "    ### run model\n",
    "    img_resized, seg_map = MODEL.run(img_orig)\n",
    "    print (img_orig.shape, img_resized.shape, seg_map.max())\n",
    "    ### Define Mask\n",
    "    img_show = img_resized.copy()\n",
    "    seg_map = np.where(seg_map == 8, 8, 0) # 예측 중 사람만 추출\n",
    "    img_mask = seg_map * (255/seg_map.max()) # 255 normalization\n",
    "    img_mask = img_mask.astype(np.uint8)\n",
    "    color_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n",
    "    img_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.35, 0.0)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    ### resize to the shape of original picture\n",
    "    img_mask_up = cv2.resize(img_mask, img_orig.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)\n",
    "    _, img_mask_up = cv2.threshold(img_mask_up, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    plt.imshow(img_mask_up, cmap=plt.cm.binary_r)\n",
    "    ax.set_title('Original Size Mask')\n",
    "\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    plt.imshow(img_mask, cmap=plt.cm.binary_r)\n",
    "    ax.set_title('DeepLab Model Mask')\n",
    "\n",
    "    plt.show()\n",
    "    ### blur picture for the background\n",
    "    img_orig_blur = cv2.blur(img_orig, (13,13)) #(13,13)은 blurring  kernel size를 뜻합니다. \n",
    "    plt.imshow(cv2.cvtColor(img_orig_blur, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    ### get blurred background\n",
    "    img_mask_color = cv2.cvtColor(img_mask_up, cv2.COLOR_GRAY2BGR)\n",
    "    img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
    "    img_bg_blur = cv2.bitwise_and(img_orig_blur, img_bg_mask)\n",
    "    plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    ### Concatenate original picture and blurred background\n",
    "    img_concat = np.where(img_mask_color==255, img_orig, img_bg_blur)\n",
    "    plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-landing",
   "metadata": {},
   "source": [
    "## Chroma Key\n",
    "\n",
    "### 1) Try Green Chroma Key\n",
    "##### generate green background and concatenate it with detected person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-detector",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### get image path\n",
    "img_dir = os.getenv('HOME')+'/aiffel/human_segmentation/images'  # 본인이 선택한 이미지의 경로에 맞게 바꿔 주세요. \n",
    "file_name= '박보영3.png' #, '박보영1.jpg',박보영2.jpg', 박보영3.png'\n",
    "print(file_name.split('.')[0])\n",
    "img_path= os.path.join(img_dir, file_name)\n",
    "img_orig = cv2.imread(img_path) \n",
    "print (img_orig.shape)\n",
    "\n",
    "### run model\n",
    "img_resized, seg_map = MODEL.run(img_orig)\n",
    "print (img_orig.shape, img_resized.shape, seg_map.max())\n",
    "\n",
    "### Define Mask\n",
    "img_show = img_resized.copy()\n",
    "seg_map = np.where(seg_map == 15, 15, 0) # 예측 중 사람만 추출\n",
    "img_mask = seg_map * (255/seg_map.max()) # 255 normalization\n",
    "img_mask = img_mask.astype(np.uint8)\n",
    "color_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n",
    "img_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.35, 0.0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "### resize to the shape of original picture\n",
    "img_mask_up = cv2.resize(img_mask, img_orig.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)\n",
    "_, img_mask_up = cv2.threshold(img_mask_up, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.imshow(img_mask_up, cmap=plt.cm.binary_r)\n",
    "ax.set_title('Original Size Mask')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "plt.imshow(img_mask, cmap=plt.cm.binary_r)\n",
    "ax.set_title('DeepLab Model Mask')\n",
    "plt.show()\n",
    "\n",
    "image_g= np.full(img_orig.shape, (0, 255, 0), dtype=np.uint8) ### give green backgroundd        \n",
    "plt.show()\n",
    "\n",
    "### get blurred background\n",
    "img_mask_color = cv2.cvtColor(img_mask_up, cv2.COLOR_GRAY2BGR)\n",
    "img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
    "img_bg_blur = cv2.bitwise_and(image_g, img_bg_mask)\n",
    "plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "### Concatenate original picture and blurred background\n",
    "img_concat = np.where(img_mask_color==255, img_orig, img_bg_blur)\n",
    "plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-burden",
   "metadata": {},
   "source": [
    "### 2) Add Different Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-express",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### get image path\n",
    "img_dir = os.getenv('HOME')+'/aiffel/human_segmentation/images'  # 본인이 선택한 이미지의 경로에 맞게 바꿔 주세요. \n",
    "file_name= 'bg1.png'\n",
    "print(file_name.split('.')[0])\n",
    "\n",
    "bg_path= os.path.join(img_dir, file_name)\n",
    "bg_orig = cv2.imread(bg_path) \n",
    "# bg_orig = cv2.cvtColor(bg_orig, cv2.COLOR_BGR2RGB) \n",
    "print (bg_orig.shape)\n",
    "plt.imshow(bg_orig)\n",
    "plt.show()\n",
    "\n",
    "# bg_= cv2.resize(bg_orig, dsize= (img_orig.shape[0], img_orig.shape[1]), interpolation=cv2.INTER_AREA)\n",
    "bg_ = cv2.resize(bg_orig, img_orig.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)\n",
    "plt.imshow(bg_)\n",
    "plt.show()\n",
    "### Concatenate original picture and blurred background\n",
    "img_concat = np.where(img_mask_color==255, img_orig, bg_)\n",
    "plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-federal",
   "metadata": {},
   "source": [
    "## 회고\n",
    "### Project summary\n",
    "#### 1) 사람 사진에서 배경을 블러처리: \n",
    "    - 사진에서 사람을 detect & segmentation, 후 블러 처리한 배경을 합성\n",
    "\n",
    "#### 2) 고양이 사진에서 배경을 블러처리: \n",
    "    - 사진에서 사람을 detect & segmentation, 후 블러 처리한 배경을 합성\n",
    "\n",
    "#### 3) 크로마키\n",
    "     - 사진에서 사람을 detect & segmentation 후 다른 배경을 합성\n",
    "\n",
    "\n",
    "### Project Process \n",
    "1) DeepLabModel 와 pretrained weight 사용하여 모델 load\n",
    "2) Define 된 Label 에서 찾고자 하는 물체를 선정하여 물체 인식, segment 하여 Mask 이미지 생성\n",
    "3) 원본 이미지를 블러처리하여 물체부분을 지움\n",
    "4) 블러 처리한 배경과 본 이미지을 합성하여 물체는 원본값, 배경은 블러 처리된 이미지 값으로 표현\n",
    "\n",
    "### Issues to be solved\n",
    "이미지에서 detect 를 부분부분 명확하게 하지 못하는 부분이 있음.\n",
    "- 사람의 사진에서는 특히 옷의 주름부분이나 머리카락 등 애매한 부분, 명암/색상차이 혹은 그림자나 옷 문양으로 인해 인식하지 못하는 부분이 있다.\n",
    "- 첫번째 사진의 경우 사람이 꽃다발을 들고 있는 부분에서, 사람 앞에 있는 물체는 명확히 사람이 아닌 것으로 잘 인식하였다. \n",
    "- 첫번째 고양이 사진에서는 고양이 앞의 꽃을 한개는 배경으로 인식하였으나, 한개는 고양이의 일부분으로 잘못 인식하여 블러처리가 되지 않았다.\n",
    "- 두번째 고양이 사진에서는 배경사진과 색상이 비슷하여 특히 꼬리 부분을 인식하지 못하여 배경과 함께 블러처리 되었다.\n",
    "- 사람의 경우, 초록색 배경으로 합성하여 확인 했을 때, 사람의 일부분이 배경으로 인식된 것을 보다 명확히 볼 수 있었으며, 다른 배경과 합성할 때 잘못 합성된 것을 확인할 수 있었다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
