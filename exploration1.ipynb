{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "competitive-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-andorra",
   "metadata": {},
   "source": [
    "Data sets used for this project:\n",
    "- smallest dataset: 300 images (100 images/class) (/r_s_p/)\n",
    "- mid-sized dataset: 7092 images (about 2500 images/class) (/hjs/test) \n",
    "- largest dataset: 300 images ( ??? images/class) (/r_c_p_3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-essence",
   "metadata": {},
   "source": [
    "#### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "early-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size=(28,28)\n",
    "    count=0\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        \n",
    "        if np.shape(old_img)[:-1] != target_size:\n",
    "#             print('np.shape(old_img)[:1] ', np.shape(old_img)[:1], 'target_size ', target_size)\n",
    "            new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "            new_img.save(img, \"JPEG\")\n",
    "            count +=1\n",
    "    print(len(images), \" images checked.\")\n",
    "    print(count, \" images resized.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "secret-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_path, number_of_data):  # 가위바위보 이미지 개수 총합에 주의하세요.  \n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    return imgs, labels, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "immediate-finger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3100  images checked.\n",
      "0  images resized.\n",
      "rock  resizing DONE!\n",
      "3100  images checked.\n",
      "0  images resized.\n",
      "scissor  resizing DONE!\n",
      "3100  images checked.\n",
      "0  images resized.\n",
      "paper  resizing DONE!\n"
     ]
    }
   ],
   "source": [
    "## RESIZE TRAINING DATA\n",
    "classes= ['rock', 'scissor', 'paper'] ## 3 classes for this project\n",
    "# 각 클래스 별로 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들이고, resize 가 필요한 파일에 대하여 resize 진행\n",
    "for class_name in classes:\n",
    "    image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock_scissor_paper/\" + class_name\n",
    "    resize_images(image_dir_path)\n",
    "\n",
    "    print(class_name, \" resizing DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-canberra",
   "metadata": {},
   "source": [
    "#### Load SEPARATE test dataset (Extra data from LMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "renewable-prime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200  images checked.\n",
      "0  images resized.\n",
      "rock  resizing DONE!\n",
      "200  images checked.\n",
      "0  images resized.\n",
      "scissor  resizing DONE!\n",
      "200  images checked.\n",
      "0  images resized.\n",
      "paper  resizing DONE!\n",
      "테스트 데이터(x_test)의 이미지 개수는 600 입니다.\n"
     ]
    }
   ],
   "source": [
    "classes= ['rock', 'scissor', 'paper']\n",
    "for class_name in classes:\n",
    "    image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/hjs/test/\" + class_name\n",
    "    resize_images(image_dir_path)\n",
    "\n",
    "    print(class_name, \" resizing DONE!\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/hjs/test\"\n",
    "(test_data, test_label, idx)=load_data(image_dir_path, 600)\n",
    "test_data_norm = test_data/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "print(\"테스트 데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-filename",
   "metadata": {},
   "source": [
    "#### BUILD MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-japanese",
   "metadata": {},
   "source": [
    "###### Set hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chronic-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channel_1=128\n",
    "n_channel_2=246\n",
    "n_channel_3=512\n",
    "n_dense=32\n",
    "n_train_epoch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "legitimate-runner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 246)       283638    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 246)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6150)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                196832    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 484,153\n",
      "Trainable params: 484,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model with two convolution layers\n",
    "\n",
    "model2=keras.models.Sequential()\n",
    "model2.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model2.add(keras.layers.MaxPool2D(2,2))\n",
    "model2.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model2.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model2.add(keras.layers.Flatten())\n",
    "model2.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model2.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "defensive-resort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 128)       3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 246)       283638    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 246)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 512)         1134080   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,437,817\n",
      "Trainable params: 1,437,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model with three convolution layers\n",
    "\n",
    "model3=keras.models.Sequential()\n",
    "model3.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model3.add(keras.layers.MaxPool2D(2,2))\n",
    "model3.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model3.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model3.add(keras.layers.Conv2D(n_channel_3, (3,3), activation='relu'))\n",
    "model3.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model3.add(keras.layers.Flatten())\n",
    "model3.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model3.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tired-communications",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 128)       3584      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 246)       283638    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 246)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 512)       1134080   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                262176    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 4,043,385\n",
      "Trainable params: 4,043,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model with four convolution layers\n",
    "\n",
    "model4=keras.models.Sequential()\n",
    "model4.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model4.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model4.add(keras.layers.MaxPool2D(2,2))\n",
    "model4.add(keras.layers.Conv2D(n_channel_3, (3,3), activation='relu'))\n",
    "model4.add(keras.layers.Conv2D(n_channel_3, (3,3), activation='relu'))\n",
    "model4.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model4.add(keras.layers.Flatten())\n",
    "model4.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model4.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-factor",
   "metadata": {},
   "source": [
    "## using 300 imgs (my own imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "smart-slope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 이미지 개수는 300 입니다.\n",
      "학습 데이터(x_train)의 이미지 개수는 240 입니다.\n"
     ]
    }
   ],
   "source": [
    "# 300 imgs Load data and generate x_train & y_train dataset\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/hjs\"\n",
    "(data, label, idx)=load_data(image_dir_path, 300)\n",
    "\n",
    "# 입력은 0~1 사이의 값으로 정규화\n",
    "data_norm = data/255.0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_norm, label, test_size=0.2, random_state= 0)\n",
    "\n",
    "print(\"총 데이터 이미지 개수는\", idx,\"입니다.\")\n",
    "print(\"학습 데이터(x_train)의 이미지 개수는\", len(X_train),\"입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-fraud",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "comparable-advisory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 6s 338ms/step - loss: 1.1437 - accuracy: 0.2957\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0950 - accuracy: 0.3988\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0871 - accuracy: 0.4227\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0761 - accuracy: 0.6812\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0537 - accuracy: 0.6172\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.0137 - accuracy: 0.4069\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.9441 - accuracy: 0.6948\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.8256 - accuracy: 0.6729\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6848 - accuracy: 0.7612\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5574 - accuracy: 0.8960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8afc871f90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 300 training imgs & model2\n",
    "model2.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model2.fit(X_train, y_train, epochs= n_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "computational-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 3s 182ms/step - loss: 1.1147 - accuracy: 0.2988\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.0983 - accuracy: 0.3077\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.0957 - accuracy: 0.3354\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.0899 - accuracy: 0.3380\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.0717 - accuracy: 0.3629\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.0345 - accuracy: 0.5540\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.9562 - accuracy: 0.5486\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.8067 - accuracy: 0.7051\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.7070 - accuracy: 0.6445\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5844 - accuracy: 0.7785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8afc385090>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 300 training imgs & model3\n",
    "model3.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model3.fit(X_train, y_train, epochs= n_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "promising-receiver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 11s 609ms/step - loss: 39.7244 - accuracy: 0.3285\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 1.3428 - accuracy: 0.3336\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 1.1571 - accuracy: 0.3338\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 1.1181 - accuracy: 0.3305\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 1.1284 - accuracy: 0.4207\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 32ms/step - loss: 1.0717 - accuracy: 0.3737\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.9816 - accuracy: 0.5443\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.6948 - accuracy: 0.7350\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.5076 - accuracy: 0.8051\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 0.3414 - accuracy: 0.8656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b79d95410>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 300 training imgs & model4\n",
    "model4.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model4.fit(data, label, epochs= n_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-antibody",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "union-benefit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 943ms/step - loss: 0.4421 - accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# split data & model2\n",
    "test_loss, test_accuracy= model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "female-contractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 565ms/step - loss: 0.6446 - accuracy: 0.6500\n"
     ]
    }
   ],
   "source": [
    "# split data & model3\n",
    "test_loss, test_accuracy= model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "thirty-banking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 2s/step - loss: 1.0841 - accuracy: 0.4667\n"
     ]
    }
   ],
   "source": [
    "# split data & model4\n",
    "test_loss, test_accuracy= model4.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "approximate-struggle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터(x_test)의 이미지 개수는 600 입니다.\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset (Extra data from LMS)\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/hjs/test\"\n",
    "(test_data, test_label, idx)=load_data(image_dir_path, 600)\n",
    "test_data_norm = test_data/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "print(\"테스트 데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "norwegian-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 58ms/step - loss: 436.4802 - accuracy: 0.4150\n"
     ]
    }
   ],
   "source": [
    "# separate data & model2\n",
    "test_loss, test_accuracy= model2.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "impressive-clark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 34ms/step - loss: 371.5153 - accuracy: 0.4117\n"
     ]
    }
   ],
   "source": [
    "# separate data & model3\n",
    "test_loss, test_accuracy= model3.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "basic-brighton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 2s 113ms/step - loss: 6.0279 - accuracy: 0.3917\n"
     ]
    }
   ],
   "source": [
    "# separate data & model4\n",
    "test_loss, test_accuracy= model4.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-efficiency",
   "metadata": {},
   "source": [
    "# using 7092 imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "medieval-incidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 이미지 개수는 7092 입니다.\n",
      "학습 데이터(x_train)의 이미지 개수는 5673 입니다.\n"
     ]
    }
   ],
   "source": [
    "# 7092 imgs load data and generate x_train & y_train dataset\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_tr, y_tr, idx)=load_data(image_dir_path, 7092)\n",
    "\n",
    "x_tr_norm = x_tr/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_tr_norm, y_tr, test_size=0.2, random_state= 0)\n",
    "\n",
    "print(\"총 데이터 이미지 개수는\", idx,\"입니다.\")\n",
    "print(\"학습 데이터(x_train)의 이미지 개수는\", len(X_train),\"입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "technical-think",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터(x_trest)의 이미지 개수는 600 입니다.\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/hjs/test\"\n",
    "(test_data, test_label, idx)=load_data(image_dir_path, 600)\n",
    "test_data_norm = test_data/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "print(\"테스트 데이터(x_trest)의 이미지 개수는\", idx,\"입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-advice",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "spread-basin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "178/178 [==============================] - 4s 22ms/step - loss: 1.1677 - accuracy: 0.4134\n",
      "Epoch 2/10\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 0.7218 - accuracy: 0.7034\n",
      "Epoch 3/10\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 0.3896 - accuracy: 0.8587\n",
      "Epoch 4/10\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 0.2032 - accuracy: 0.9348\n",
      "Epoch 5/10\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 0.1221 - accuracy: 0.9664\n",
      "Epoch 6/10\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 0.0932 - accuracy: 0.9753\n",
      "Epoch 7/10\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 0.1743 - accuracy: 0.9460\n",
      "Epoch 8/10\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 0.0376 - accuracy: 0.9921\n",
      "Epoch 9/10\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 0.0330 - accuracy: 0.9905\n",
      "Epoch 10/10\n",
      "178/178 [==============================] - 1s 7ms/step - loss: 0.0178 - accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b79000dd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 7092 training imgs & model2\n",
    "model2.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model2.fit(X_train, y_train, epochs= n_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "helpful-harbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "178/178 [==============================] - 4s 21ms/step - loss: 1.1770 - accuracy: 0.4252\n",
      "Epoch 2/10\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.6919 - accuracy: 0.6956\n",
      "Epoch 3/10\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.4250 - accuracy: 0.8262\n",
      "Epoch 4/10\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.3019 - accuracy: 0.8883\n",
      "Epoch 5/10\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.1680 - accuracy: 0.9457\n",
      "Epoch 6/10\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.1157 - accuracy: 0.9596\n",
      "Epoch 7/10\n",
      "178/178 [==============================] - 2s 12ms/step - loss: 0.1014 - accuracy: 0.9677\n",
      "Epoch 8/10\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.0615 - accuracy: 0.9824\n",
      "Epoch 9/10\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.0586 - accuracy: 0.9813\n",
      "Epoch 10/10\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 0.0457 - accuracy: 0.9884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b78eec610>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 7092 training imgs & model3\n",
    "model3.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model3.fit(X_train, y_train, epochs= n_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "accessory-toolbox",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "178/178 [==============================] - 12s 67ms/step - loss: 1.0718 - accuracy: 0.4308\n",
      "Epoch 2/10\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 0.7955 - accuracy: 0.6553\n",
      "Epoch 3/10\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 0.3776 - accuracy: 0.8527\n",
      "Epoch 4/10\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 0.2061 - accuracy: 0.9306\n",
      "Epoch 5/10\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 0.1127 - accuracy: 0.9645\n",
      "Epoch 6/10\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 0.0593 - accuracy: 0.9816\n",
      "Epoch 7/10\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 0.0238 - accuracy: 0.9926\n",
      "Epoch 8/10\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 0.0158 - accuracy: 0.9960\n",
      "Epoch 9/10\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 0.1109 - accuracy: 0.9687\n",
      "Epoch 10/10\n",
      "178/178 [==============================] - 6s 33ms/step - loss: 0.0206 - accuracy: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b78e11550>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 7092 training imgs & model4\n",
    "model4.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model4.fit(X_train, y_train, epochs= n_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-remainder",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "heavy-twelve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 2s 36ms/step - loss: 0.1206 - accuracy: 0.9591\n",
      "test_loss: 0.12056318670511246 \n",
      "test_accuracy: 0.9591261744499207\n"
     ]
    }
   ],
   "source": [
    "# split data & model2\n",
    "test_loss, test_accuracy= model2.evaluate(X_test, y_test)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "rental-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 22ms/step - loss: 0.0677 - accuracy: 0.9789\n"
     ]
    }
   ],
   "source": [
    "# split data & model3\n",
    "test_loss, test_accuracy= model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "lesbian-bathroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 3s 64ms/step - loss: 0.0309 - accuracy: 0.9887\n"
     ]
    }
   ],
   "source": [
    "# split data & model4\n",
    "test_loss, test_accuracy= model4.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "amber-bible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 4ms/step - loss: 420.6739 - accuracy: 0.4883\n"
     ]
    }
   ],
   "source": [
    "# separate data & model2\n",
    "test_loss, test_accuracy= model2.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "tracked-oxford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 5ms/step - loss: 503.0591 - accuracy: 0.4933\n"
     ]
    }
   ],
   "source": [
    "# separate data & model3\n",
    "test_loss, test_accuracy= model3.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "typical-andrews",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 11ms/step - loss: 364.4283 - accuracy: 0.5583\n"
     ]
    }
   ],
   "source": [
    "# separate data & model4\n",
    "test_loss, test_accuracy= model4.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-theology",
   "metadata": {},
   "source": [
    "### 16392   DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "signal-marriage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 이미지 개수는 16392 입니다.\n",
      "학습 데이터(x_train)의 이미지 개수는 13113 입니다.\n",
      "test 데이터(x_test)의 이미지 개수는 3279 입니다.\n"
     ]
    }
   ],
   "source": [
    "## 16392  imgs load data and generate x_train & y_train dataset\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_tr, y_tr, idx)=load_data(image_dir_path, 7092)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock_scissor_paper/\" + class_name\n",
    "(x_tr_1, y_tr_1, idx)=load_data(image_dir_path, 9300)\n",
    "\n",
    "x_tr_data= np.vstack((x_tr, x_tr_1)) ## total 13113 images\n",
    "y_tr_data= np.append(y_tr, y_tr_1)\n",
    "\n",
    "x_tr_data_norm = x_tr_data/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_tr_data_norm, y_tr_data, test_size=0.2, random_state= 0)\n",
    "\n",
    "print(\"총 데이터 이미지 개수는\", len(y_tr_data) ,\"입니다.\")\n",
    "print(\"학습 데이터(x_train)의 이미지 개수는\", len(X_train),\"입니다.\")\n",
    "print(\"test 데이터(x_test)의 이미지 개수는\", len(X_test),\"입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "novel-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터(x_trest)의 이미지 개수는 600 입니다.\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/hjs/test\"\n",
    "(test_data, test_label, idx)=load_data(image_dir_path, 600)\n",
    "test_data_norm = test_data/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "print(\"테스트 데이터(x_trest)의 이미지 개수는\", idx,\"입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-labor",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "immune-pollution",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "410/410 [==============================] - 7s 16ms/step - loss: 0.1118 - accuracy: 0.9702\n",
      "Epoch 2/10\n",
      "410/410 [==============================] - 3s 7ms/step - loss: 0.0164 - accuracy: 0.9960\n",
      "Epoch 3/10\n",
      "410/410 [==============================] - 3s 7ms/step - loss: 0.0208 - accuracy: 0.9941\n",
      "Epoch 4/10\n",
      "410/410 [==============================] - 3s 7ms/step - loss: 0.0458 - accuracy: 0.9857\n",
      "Epoch 5/10\n",
      "410/410 [==============================] - 3s 7ms/step - loss: 0.0166 - accuracy: 0.9951\n",
      "Epoch 6/10\n",
      "410/410 [==============================] - 3s 7ms/step - loss: 0.0051 - accuracy: 0.9988\n",
      "Epoch 7/10\n",
      "410/410 [==============================] - 3s 7ms/step - loss: 0.0223 - accuracy: 0.9927\n",
      "Epoch 8/10\n",
      "410/410 [==============================] - 3s 7ms/step - loss: 0.0134 - accuracy: 0.9964\n",
      "Epoch 9/10\n",
      "410/410 [==============================] - 3s 7ms/step - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 10/10\n",
      "410/410 [==============================] - 3s 7ms/step - loss: 8.3494e-04 - accuracy: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b78b2f3d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 16392 training imgs & model2\n",
    "model2.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model2.fit(X_train, y_train, epochs= n_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fitted-juice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "410/410 [==============================] - 7s 17ms/step - loss: 0.0443 - accuracy: 0.9848\n",
      "Epoch 2/10\n",
      "410/410 [==============================] - 5s 11ms/step - loss: 0.0370 - accuracy: 0.9862\n",
      "Epoch 3/10\n",
      "410/410 [==============================] - 5s 11ms/step - loss: 0.0407 - accuracy: 0.9880\n",
      "Epoch 4/10\n",
      "410/410 [==============================] - 5s 11ms/step - loss: 0.0216 - accuracy: 0.9932\n",
      "Epoch 5/10\n",
      "410/410 [==============================] - 5s 11ms/step - loss: 0.0361 - accuracy: 0.9880\n",
      "Epoch 6/10\n",
      "410/410 [==============================] - 5s 11ms/step - loss: 0.0129 - accuracy: 0.9960\n",
      "Epoch 7/10\n",
      "410/410 [==============================] - 5s 11ms/step - loss: 0.0376 - accuracy: 0.9881\n",
      "Epoch 8/10\n",
      "410/410 [==============================] - 5s 11ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 9/10\n",
      "410/410 [==============================] - 5s 11ms/step - loss: 0.0101 - accuracy: 0.9973\n",
      "Epoch 10/10\n",
      "410/410 [==============================] - 5s 11ms/step - loss: 0.0228 - accuracy: 0.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b1c1e1e90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 16392 training imgs & model3\n",
    "model3.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model3.fit(X_train, y_train, epochs= n_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "attended-warehouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "410/410 [==============================] - 22s 53ms/step - loss: 0.1307 - accuracy: 0.9803\n",
      "Epoch 2/10\n",
      "410/410 [==============================] - 14s 33ms/step - loss: 0.0080 - accuracy: 0.9970\n",
      "Epoch 3/10\n",
      "410/410 [==============================] - 14s 33ms/step - loss: 0.0218 - accuracy: 0.9939\n",
      "Epoch 4/10\n",
      "410/410 [==============================] - 14s 33ms/step - loss: 0.0217 - accuracy: 0.9942\n",
      "Epoch 5/10\n",
      "410/410 [==============================] - 14s 33ms/step - loss: 0.0099 - accuracy: 0.9968\n",
      "Epoch 6/10\n",
      "410/410 [==============================] - 14s 33ms/step - loss: 0.0144 - accuracy: 0.9976\n",
      "Epoch 7/10\n",
      "410/410 [==============================] - 14s 33ms/step - loss: 0.0088 - accuracy: 0.9979\n",
      "Epoch 8/10\n",
      "410/410 [==============================] - 14s 33ms/step - loss: 0.0134 - accuracy: 0.9965\n",
      "Epoch 9/10\n",
      "410/410 [==============================] - 14s 33ms/step - loss: 0.0064 - accuracy: 0.9984\n",
      "Epoch 10/10\n",
      "410/410 [==============================] - 14s 33ms/step - loss: 0.0014 - accuracy: 0.9995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b1c0dc590>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 16392 training imgs & model4\n",
    "model4.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model4.fit(X_train, y_train, epochs= n_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-monaco",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "mental-produce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 3s 24ms/step - loss: 0.0124 - accuracy: 0.9970\n"
     ]
    }
   ],
   "source": [
    "# split data & model2\n",
    "test_loss, test_accuracy= model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "corporate-denmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 2s 14ms/step - loss: 0.0269 - accuracy: 0.9939\n"
     ]
    }
   ],
   "source": [
    "# split data & model3\n",
    "test_loss, test_accuracy= model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "above-fetish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 4s 41ms/step - loss: 0.0113 - accuracy: 0.9970\n"
     ]
    }
   ],
   "source": [
    "# split data & model4\n",
    "test_loss, test_accuracy= model4.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "leading-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 4ms/step - loss: 673.7531 - accuracy: 0.4700\n"
     ]
    }
   ],
   "source": [
    "# separate data & model2\n",
    "test_loss, test_accuracy= model2.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "olympic-consultancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 5ms/step - loss: 214.3754 - accuracy: 0.6583\n"
     ]
    }
   ],
   "source": [
    "# separate data & model3\n",
    "test_loss, test_accuracy= model3.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "moved-homework",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 11ms/step - loss: 295.4477 - accuracy: 0.5717\n"
     ]
    }
   ],
   "source": [
    "# separate data & model4\n",
    "test_loss, test_accuracy= model4.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-coalition",
   "metadata": {},
   "source": [
    "## 7092 raw data (not normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "renewable-heaven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터(x_train)의 이미지 개수는 5673 입니다.\n",
      "테스트 데이터(x_trest)의 이미지 개수는 600 입니다.\n"
     ]
    }
   ],
   "source": [
    "### 정규화 하지 않은 데이터 생성\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_tr, y_tr, test_size=0.2, random_state= 0)\n",
    "\n",
    "print(\"학습 데이터(x_train)의 이미지 개수는\", len(X_train),\"입니다.\")\n",
    "\n",
    "# SEPARATE test dataset 도 정규화 하지 않은 데이터 생성\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/hjs/test\"\n",
    "(test_data, test_label, idx)=load_data(image_dir_path, 600)\n",
    "print(\"테스트 데이터(x_trest)의 이미지 개수는\", idx,\"입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "loved-access",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "222/222 [==============================] - 6s 27ms/step - loss: 10.7833 - accuracy: 0.9822\n",
      "Epoch 2/10\n",
      "222/222 [==============================] - 2s 7ms/step - loss: 5.6605 - accuracy: 0.9855\n",
      "Epoch 3/10\n",
      "222/222 [==============================] - 2s 7ms/step - loss: 2.8481 - accuracy: 0.9877\n",
      "Epoch 4/10\n",
      "222/222 [==============================] - 2s 7ms/step - loss: 0.8657 - accuracy: 0.9933\n",
      "Epoch 5/10\n",
      "222/222 [==============================] - 2s 7ms/step - loss: 0.9001 - accuracy: 0.9927\n",
      "Epoch 6/10\n",
      "222/222 [==============================] - 2s 7ms/step - loss: 0.6314 - accuracy: 0.9959\n",
      "Epoch 7/10\n",
      "222/222 [==============================] - 2s 7ms/step - loss: 0.1932 - accuracy: 0.9978\n",
      "Epoch 8/10\n",
      "222/222 [==============================] - 2s 7ms/step - loss: 0.6663 - accuracy: 0.9929\n",
      "Epoch 9/10\n",
      "222/222 [==============================] - 2s 7ms/step - loss: 3.1187 - accuracy: 0.9804\n",
      "Epoch 10/10\n",
      "222/222 [==============================] - 2s 7ms/step - loss: 2.8275 - accuracy: 0.9855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b1bf792d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model2.fit(x_tr, y_tr, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "golden-substance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "222/222 [==============================] - 6s 24ms/step - loss: 14.6525 - accuracy: 0.9661\n",
      "Epoch 2/10\n",
      "222/222 [==============================] - 3s 11ms/step - loss: 2.6326 - accuracy: 0.9887\n",
      "Epoch 3/10\n",
      "222/222 [==============================] - 3s 11ms/step - loss: 1.1374 - accuracy: 0.9902\n",
      "Epoch 4/10\n",
      "222/222 [==============================] - 3s 11ms/step - loss: 0.7465 - accuracy: 0.9927\n",
      "Epoch 5/10\n",
      "222/222 [==============================] - 3s 11ms/step - loss: 0.2100 - accuracy: 0.9942\n",
      "Epoch 6/10\n",
      "222/222 [==============================] - 3s 11ms/step - loss: 0.9872 - accuracy: 0.9892\n",
      "Epoch 7/10\n",
      "222/222 [==============================] - 3s 11ms/step - loss: 0.7818 - accuracy: 0.9919\n",
      "Epoch 8/10\n",
      "222/222 [==============================] - 3s 11ms/step - loss: 0.0316 - accuracy: 0.9968\n",
      "Epoch 9/10\n",
      "222/222 [==============================] - 3s 12ms/step - loss: 0.0198 - accuracy: 0.9985\n",
      "Epoch 10/10\n",
      "222/222 [==============================] - 3s 11ms/step - loss: 0.0044 - accuracy: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8affc38350>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 7092 training imgs - not normalized\n",
    "model3.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model3.fit(x_tr, y_tr, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "developing-danger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "222/222 [==============================] - 17s 74ms/step - loss: 7.0935 - accuracy: 0.9839\n",
      "Epoch 2/10\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.5959 - accuracy: 0.9922\n",
      "Epoch 3/10\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.7760 - accuracy: 0.9957\n",
      "Epoch 4/10\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.2619 - accuracy: 0.9977\n",
      "Epoch 5/10\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.0789 - accuracy: 0.9938\n",
      "Epoch 6/10\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 1.0663 - accuracy: 0.9901\n",
      "Epoch 7/10\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.8592 - accuracy: 0.9873\n",
      "Epoch 8/10\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.1996 - accuracy: 0.9976\n",
      "Epoch 9/10\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.1852 - accuracy: 0.9954\n",
      "Epoch 10/10\n",
      "222/222 [==============================] - 7s 33ms/step - loss: 0.0024 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8affb5f3d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### 7092 training imgs - not normalized\n",
    "model4.compile(optimizer='adam',\n",
    "                 loss= 'sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "model4.fit(x_tr, y_tr, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "final-variable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 4ms/step - loss: 0.5683 - accuracy: 0.9930\n"
     ]
    }
   ],
   "source": [
    "# split data & model2\n",
    "test_loss, test_accuracy= model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "signal-optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 5ms/step - loss: 1.8903e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# split data & model3\n",
    "test_loss, test_accuracy= model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "speaking-albania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 11ms/step - loss: 0.5215 - accuracy: 0.9887\n"
     ]
    }
   ],
   "source": [
    "# split data & model4\n",
    "test_loss, test_accuracy= model4.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "generous-omaha",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 4ms/step - loss: 190.0203 - accuracy: 0.5383\n"
     ]
    }
   ],
   "source": [
    "# separate data & model2\n",
    "test_loss, test_accuracy= model2.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cardiac-provision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 5ms/step - loss: 66.8886 - accuracy: 0.6583\n"
     ]
    }
   ],
   "source": [
    "# separate data & model3\n",
    "test_loss, test_accuracy= model3.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "chubby-massage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 11ms/step - loss: 75.7625 - accuracy: 0.5550\n"
     ]
    }
   ],
   "source": [
    "# separate data & model4\n",
    "test_loss, test_accuracy= model4.evaluate(test_data, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-chart",
   "metadata": {},
   "source": [
    "### 회고\n",
    "\n",
    "#### Dataset\n",
    "- 가위, 바위, 보 3가지의 클래스를 분류하는 모델에 대하여, 3가지 종류의 데이터를 사용하여 각각 학습하였다. \n",
    "- Training Dataset: \n",
    "    각 데이터셋은 이미지 갯수에 차이가 있으며, 중복된 데이터가 있을 가능성이 있다.\n",
    "    - smallest dataset: 300 images: 내가 찍은 각 100장, 총 300장의 이미지를 사용하였다.\n",
    "    - mid-sized dataset: 7092 images: 각 학생들이 올려 공유한 데이터 중 일부를 통합하여 사용하였다.\n",
    "    - largest dataset: 16392 images: 모든 사람들이 올리고 공유한 모든 데이터를 통합하여 사용하였다.\n",
    "- Test Dataset:\n",
    "    전체 데이터를 training & test dataset 으로 나누어 test data을 확보하였으나, 여기서 확보한 test dataset 은 사실 이미지를 얻는 과정에서 웹캠이 빠른 속도로 찍으면서 중복되거나 아주 비슷한 이미지가 여러개 있을 가능성이 있으므로 training data 와 중복된 데이터가 포함될 가능성이 높다.\n",
    "    학습 된 모델의 실제 성능을 검증하기 위해서는 모델이 보지 못한 데이터로 테스트 하는것이 원칙이므로, 이 데이터셋 으로는 제대로 된 성능을 검증하기 어려울 수 있다고 판단하여 training data 에 사용하지 않은 새로운 데이터셋을 테스트 데이터셋으로 사용하였다.(이 때에는 LMS sample 에서 얻은 각 클래스별로 200장, 총 600장의 데이터셋을 사용하였다.)\n",
    "    - 아래 결과 테이블 에서는 split 하여 얻은 test data와 LMS sample 에서 얻은 test data 두가지로 나누어 시험한 것을 볼 수 있다.\n",
    "    \n",
    "    \n",
    "#### Preprocessing\n",
    "- 모든 이미지를 28x28x3 으로 resize 하여 사용하였다.\n",
    "- 모든 이미지를 255 로 나누어 normalize 하여 사용하였다.\n",
    "\n",
    "\n",
    "#### Hyperparameter\n",
    "- 컨볼루션 레이어의 갯수를 2, 3, 4 로 키워 학습시켜보았으며, 각 컨볼루션 레이어의 hyperparameter 를 128, 64, 32, 16 으로 세팅, dense layer 는 32, 마지막 dense layer 는 클래스의 갯수에 맞추어 3으로 맞춰주었다.\n",
    "- 각 모델별 epoch 수는 3, 5, 7, 10 으로 학습시켜 비교하였다.\n",
    "\n",
    "(validation - accuracy 가 안나와 cross-validation 을 해보고자 StratifiedKFold 를 사용하였으며, split 갯수는 3, 4, 5 로 테스트 하였다.\n",
    "\n",
    "\n",
    "#### Evaluation\n",
    "- 300개의 이미지만 사용하였을 때에는 split 데이터 사용시 최고성능 88.33%, 새로운 데이터 사용시 최고성능 41.5%로 성능이 매우 낮음을 볼 수 있다. \n",
    "- 7092개의 이미지를 사용하여 학습시켰을 때에는 split 데이터로 test 시 최고성능 98%, 새로운 데이터로 test 시 최고성능 55.8% 로, 300개로 학습시켰을 때 보다 각 모델 별 혹은 test data별로 10~20% 정도의 성능 향상이 있었다.\n",
    "- 16392개의 이미지를 사용하여 학습 시켰을 때에는 7092 개의 데이터를 사용하였을 보다 조금 높아지며, 3 레이어를 사용하였 을 때 65.83% 로 성능 향상이 있었다. \n",
    "- 큰 데이터셋으로 학습한 모델들이 새로운 데이터셋으로 테스트 했을 때, accuracy가 높아졌지만, 이와 동시에 loss 값이 함께 올라갔다. \n",
    "\n",
    "##### Result\n",
    "실험 결과를 통해\n",
    "1. split data 로 test 했을 때에는 90% 이상이 나오지만 separate 데이터를 사용하였을 때에는 40~50% 인 것으로 보아 우리가 사용한 데이터에는 비슷한 데이터가 많아 train_test_split 을 사용하여 나눈 test data 는 train data 와 중복성이 있을 것으로 예상할 수 있다.\n",
    "2. 데이터를 많이 사용하였을 때 데이터가 적을 때 보다 모델 성능이 향상됨을 볼 수 있다.\n",
    "3. 모델의 성능을 안정적으로 높이기 위해서는 loss 값을 줄이기 위한 다른 방향을 찾아봐야 하겠다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "driven-blink",
   "metadata": {},
   "source": [
    "|     No. of Data   |  No. of Conv Layers|  split | separate |                                  Remarks                                  |\n",
    "|:-----------------:|:------------------:|:------:|:--------:|:--------------------------------------------------------------------------|\n",
    "|        300        |          2d        | 0.8    | 0.415    | > 새로운 데이터에 대해서 성능이 매우 낮음                 |\n",
    "|                   |          3d        | 0.65   | 0.4117   |                                                                           |\n",
    "|                   |          4d        | 0.4667 | 0.3917   |                                                                           |\n",
    "|        7092       |          2d        | 0.9591 | 0.4883   | > 300개 이미지로만 했을때와 비교하면  기존 데이터를 나눈 경우에도 95% 이상으로 성능이|\n",
    "|                   |          3d        | 0.9789 | 0.4933   |   매우 높고 새로운 데이터로 했을 때에도 10% 정도의 성능 향상이 있음.         |\n",
    "|                   |          4d        | 0.9887 | 0.5583   |                   |\n",
    "|       16392       |          2d        | 0.997  | 0.47     | > 7092 데이터로 했을때와 마찬가지로 split 한 데이터로 테스트 했을 때  99% 이상의 성능이 |\n",
    "|                   |          3d        | 0.9939 | 0.6583   |           나왔고 새로운 데이터로 테스트 했을 때에는 7092개로 했을 때 보다 조금씩 더 올랐다.|\n",
    "|                   |          4d        | 0.997  | 0.5717   | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-festival",
   "metadata": {},
   "source": [
    "|   Hyper Parameters Used  |\n",
    "|:------------------------:|\n",
    "|     n_channel_1 = 128    |\n",
    "|     n_channel_2 = 246    |\n",
    "|     n_channel_3 = 512    |\n",
    "|       n_dense = 32       |\n",
    "|    n_train_epoch = 10    |\n",
    "| train & test split : 20% |\n",
    "|     Random state = 0     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-judges",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
