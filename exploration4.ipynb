{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attractive-nebraska",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "joined-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-medicaid",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conscious-charter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 모든 가사파일을 raw_corpus 에 넣음.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-vision",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "periodic-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=12000\n",
    "random_state= 0\n",
    "\n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "batch_size= 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-spare",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "#### Clean data by removeing unnecessary parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imported-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1. 소문자로 바꾸고, 양쪽 공백 제거\n",
    "    sentence = re.sub('[\\[]+[a-zA-Z0-9\\s]+[\\]]', '',sentence) ## 대괄호에 쓰여있는 instruction 제거\n",
    "#     sentence = re.sub('[\\(+[a-zA-Z0-9\\s]+[\\)]', '',sentence) ## 괄호에 쓰여있는 instruction 제거\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2. 특수문자 양쪽에 공백추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3. 여러개의 공백을 하나의 공백으로 변경\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9?.!,¿\\[\\]]+\", \" \", sentence) # 4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 변경\n",
    "    sentence = sentence.strip() # 5. 양쪽 공백 제거\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6. 문장 시작에 <start>, 끝에 <end>를 추가\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-powder",
   "metadata": {},
   "source": [
    "#### remove sentences having more that 15 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "floral-diversity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> there must be some kind of way outta here <end>',\n",
       " '<start> said the joker to the thief <end>',\n",
       " '<start> there s too much confusion <end>',\n",
       " '<start> i can t get no relief business men , they drink my wine <end>',\n",
       " '<start> plowman dig my earth <end>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue ## 빈 문장 제거\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    pre_list = preprocessed_sentence.split(' ') ## 정제 후 빈 문장 제거\n",
    "    if len(pre_list) <= 3:  \n",
    "        pre_list.remove('<start>')\n",
    "        pre_list.remove('<end>')\n",
    "        if pre_list== ['']: continue\n",
    "            \n",
    "#     if preprocessed_sentence in corpus: print(preprocessed_sentence) # continue ## 정제 후 이미 같은 문장이 corpus 에 있으면 추가하지 않음\n",
    "    \n",
    "    if len(preprocessed_sentence.split(' ')) > 17 : continue ## 17 단어보다 긴 문장 추가하지 않음 (15단어 + <start>, <end>)\n",
    "        \n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "painted-behavior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187088"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "occupied-terrorism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163203"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adaptive-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15개 보다 많은 단어로 된 문장이 제거 되었는지 확인 (start, end 포함시에는 17로 변경하여 확인)\n",
    "for sentence in corpus:\n",
    "    if len(sentence.split(' ')) > 17:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-determination",
   "metadata": {},
   "source": [
    "#### 텍스트 데이터를 토큰화 하여 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "liberal-devil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  62 276 ...   0   0   0]\n",
      " [  2 113   6 ...   0   0   0]\n",
      " [  2  62  17 ...   0   0   0]\n",
      " ...\n",
      " [  2  76  47 ...   0   0   0]\n",
      " [  2  49   5 ...   0   0   0]\n",
      " [  2  13 653 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fbc0d0f4790>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=num_words,  # 12000단어를 기억할 수 있는 tokenizer를 생성\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\" ## 12000단어에 포함되지 못한 단어는 '<unk>'로 변경 (unknown)\n",
    "    )\n",
    "    \n",
    "    ## corpus를 이용해 tokenizer 내부의 단어장에 저장\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    ## tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # padding 을 사용하여 입력 데이터의 시퀀스 길이를 일정하게 맞춤\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-earthquake",
   "metadata": {},
   "source": [
    "#### 위에서 만든 tensor를  source 와  target 데이터로 분리 (data and label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "operational-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tensor[:, :-1]  ## tensor에서 마지막 토큰 삭제\n",
    "tgt_input = tensor[:, 1:]  ## tensor에서 <start>를 잘라내어 타겟 문장 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-danish",
   "metadata": {},
   "source": [
    "#### split into train & validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "going-teaching",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=random_state )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-slovakia",
   "metadata": {},
   "source": [
    "##### check train&test data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "interstate-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (130562, 16)\n",
      "Target Train: (130562, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-converter",
   "metadata": {},
   "source": [
    "### Generate Dataset\n",
    "- tf.data.Dataset 사용\n",
    "- train & validation 두가지 dataset 객체를 각각 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "apparent-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "steps_per_epoch = len(src_input) // batch_size\n",
    "\n",
    "## tokenizer가 구축한 단어사전 12000개에 0:<pad>를 포함하여 12001개가 됨.\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "## training dataset\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset_train = dataset_train.shuffle(BUFFER_SIZE)\n",
    "dataset_train = dataset_train.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "## validation dataset\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "dataset_val = dataset_val.shuffle(BUFFER_SIZE)\n",
    "dataset_val = dataset_val.batch(batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-papua",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alone-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "industrial-forestry",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1020/1020 [==============================] - 391s 377ms/step - loss: 3.3747 - val_loss: 2.7159\n",
      "Epoch 2/10\n",
      "1020/1020 [==============================] - 389s 382ms/step - loss: 2.6404 - val_loss: 2.5560\n",
      "Epoch 3/10\n",
      "1020/1020 [==============================] - 354s 347ms/step - loss: 2.4585 - val_loss: 2.4539\n",
      "Epoch 4/10\n",
      "1020/1020 [==============================] - 334s 327ms/step - loss: 2.3173 - val_loss: 2.3814\n",
      "Epoch 5/10\n",
      "1020/1020 [==============================] - 336s 329ms/step - loss: 2.1841 - val_loss: 2.3234\n",
      "Epoch 6/10\n",
      "1020/1020 [==============================] - 333s 326ms/step - loss: 2.0677 - val_loss: 2.2803\n",
      "Epoch 7/10\n",
      "1020/1020 [==============================] - 333s 326ms/step - loss: 1.9509 - val_loss: 2.2443\n",
      "Epoch 8/10\n",
      "1020/1020 [==============================] - 335s 328ms/step - loss: 1.8498 - val_loss: 2.2206\n",
      "Epoch 9/10\n",
      "1020/1020 [==============================] - 335s 328ms/step - loss: 1.7471 - val_loss: 2.2001\n",
      "Epoch 10/10\n",
      "1020/1020 [==============================] - 333s 326ms/step - loss: 1.6617 - val_loss: 2.1908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbc0aab4e90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set optimizer and compile model\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset_train, epochs=10, validation_data= (enc_val,dec_val), batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "composite-great",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 33,133,793\n",
      "Trainable params: 33,133,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-ivory",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "postal-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    ## 입력받은 init_sentence를 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        ## 1. 입력받은 문장의 텐서를 입력\n",
    "        predict = model(test_tensor) \n",
    "        ## 2. 예측된 값 중 가장 높은 확률인 word index 선정\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        ## 3. 2에서 예측된 word index를 문장 뒤에 추가\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        ## 4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마침\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    ## tokenizer를 이용해 word index를 단어로 변환 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bridal-playlist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> only one thing i can t do <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence='<start> only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "occasional-modification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you re the only one that i know <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence='<start> you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "taken-listening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are the one that i need <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence='<start> you are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "chief-clause",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a walker in the rain <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence='<start> he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "handmade-profession",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence='<start> I love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "psychological-boost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> one and two , three , four , four , five , six , seven <end> '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence='<start> one and ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-costume",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "### Dataset\n",
    "- 49개의 가사 txt 파일을 긁어 와서 총 187,088개의 문장을 데이터를 얻었으며, 그 중 가사가 아닌 부분을 제거하여 총 163,203개 문장을 사용하였다.\n",
    "- 텍스트 데이터를 tocenize 하여 각각의 단어를 tokenizer 내부의 단어장에 저장하고, 저장된 단어들은 그에 상응하는 tensor 로 변환하였다. (모든 텐서는 패딩을 사용하여 일정한 길이로 맞추었다)\n",
    "- 토큰화하여 생성된 텐서에서 [-1]을 제거한 source 데이터와 [0]을 제거한 target 데이터를 각각 생성하였고, 이 데이터를 80%는 train dataset (130,562개), 20% 는 validation dataset (32,641개) 으로 나누어 각각의 데이터셋 객체를 생성하였다.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "- 단어장의 총 단어수는 12,000으로 제한되었으며, epoch 도 10으로 제한되어있다.\n",
    "- train_test_split 의 random_state=[0, 7, 32] 세가지와, embedding_size = [128, 256, 512], hidden_size = [512, 1024, 2048], batch_size=[32, 64, 128, 256, 512] 를 각각 실험하였다. 이 중 가장 좋은 성능을 나타낸 hyperparameter는 result 에 추가하였다.\n",
    "\n",
    "\n",
    "### Model structure\n",
    "- One embedding layer\n",
    "- Two rnn layers with LSTM\n",
    "- One dense layer\n",
    "\n",
    "\n",
    "### Analysis\n",
    "- random_state=32 로 시작했을 때에는 loss가 처음엔 6 이상에서 시작하여. 첫 epoch 에서 loss 가 3.7로 내려갔다. (val_loss 2.8) randome_state = 0 으로 변경 후에는 loss 가 애초부터 낮게 시작하여 더 빨리 로스값을 낮출 수 있었다. \n",
    "- embedding size 와 hiddel size 의 선정이 어려웠는데, 예제에서 주어졌던 값에서 위아래로 테스트 해 나가면서 성능이 잘 나오는 쪽으로 추려갔다. \n",
    "- 대체적으로 batch size 가 낮을수록 loss 값이 낮아지는 경향이 보였으나 속도가 너무 느려져서 문제가 있었다. \n",
    "- 또한, Sequential 한 signal 에서는 너무 짧은 신호는 사용하지 않는데, nlp 에서도 어느정도의 의미 있는 문장이 사용되어야 하지 않을까..? 하는 생각에서 오는 의문점으로, batch size 가 너무 낮으면 문장을 배워야 하는 모델이 너무 짧은 단어 몇개만을 배울 가능성이 있어보여서, batch size 가 얼마나 작아져도 괜찮은지, 32, 64 는 어느정도의 특징을 가지고 있는지 알아봐야 하겠다. \n",
    "- input을 주면 어느정도 말이 되는 문장을 만들어 낸다. (nlp 처음 해봤는데, 귀엽고 신기한 것 같다!)\n",
    "\n",
    "\n",
    "### Results\n",
    "- 아래의 hyperparameter 을 사용하고, 10 epoch 을 돌렸을 때, val-loss = 2.1908 이 나왔다. (2.2 이하의 loss 값이 나온 케이스가 몇가지 더 있었으나, 이 코드에서는 아래의 케이스를 선정하였다.)\n",
    "    - random_state= 0\n",
    "    - embedding_size = 512\n",
    "    - hidden_size = 1024\n",
    "    - batch_size= 128\n",
    "- 모델이 생성한 몇가지 문장은 회고 위에서 볼 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
